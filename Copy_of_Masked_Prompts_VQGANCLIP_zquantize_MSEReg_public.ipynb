{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Masked Prompts VQGANCLIP_zquantize_MSEReg public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrPhipps/Colabs/blob/main/Copy_of_Masked_Prompts_VQGANCLIP_zquantize_MSEReg_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "# Generates images from masked text prompts with VQGAN and CLIP (z+quantize and MSE reg. method).\n",
        "\n",
        "Modification by Brian Davis https://twitter.com/brian_l_d\n",
        "\n",
        "Based on jbustter's https://twitter.com/jbusted1 notebook which was based on\n",
        "a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
        "\n",
        "Go to ARGS to set the parameters!\n",
        "\n",
        "The masking is defined by a supplied RBG image (prompt key image) and you define a color for each prompt. The mask is created by looking at each pixel of the prompt key image and assigning it to the mask with the closest color.\n",
        "\n",
        "The masks are used to isolate the gradient to the specified region. You also can define how often you'd like CLIP to be \"blindfolded\" from the non-masked region (region it can't influence). This is useful becuase CLIP has a global bias which causes it to copy color/patterns from one part of the image to another, even if the prompt doesn't match that color/style."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUfzT60ZZ9q"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSfISAhyPmyp"
      },
      "source": [
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhdWrSxQhwg"
      },
      "source": [
        "\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt\n",
        "\n",
        "# Mirrors\n",
        "# !curl -L 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.yaml' > vqgan_imagenet_f16_16384.yaml\n",
        "# !curl -L 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.ckpt' > vqgan_imagenet_f16_16384.ckpt \n",
        "\n",
        "# !curl -L https://dl.nmkd.de/ai/clip/vqgan/8k-2021-06/vqgan-f8-8192.ckpt > vqgan_openimages_f16_8192.ckpt\n",
        "# !curl -L https://dl.nmkd.de/ai/clip/vqgan/8k-2021-06/vqgan-f8-8192.yaml > vqgan_openimages_f16_8192.yaml\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTbIDhe7k-wa"
      },
      "source": [
        "#Reset point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B72TCs16lpxz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "source": [
        "def noise_gen(shape):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    for i in reversed(range(5)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        cutout_coords=[]\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "          #we now add sample points from the curout region to use in looking up spatial prompts\n",
        "          cutout_coords.append([offsetx,offsetx + size,offsety,offsety + size])\n",
        "                                \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        # if args.use_augs:\n",
        "        #   cutouts = augs(cutouts)\n",
        "\n",
        "        # if args.noise_fac:\n",
        "        #   facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "        #   cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        \n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1), cutout_coords\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "\n",
        "%mkdir ./vids #/content/vids\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acct_svw6t7v"
      },
      "source": [
        "import requests, io\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "source": [
        "args = argparse.Namespace(\n",
        "    \n",
        "    #spatial_prompts is a list of tuples (color, blindfold_prob, prompt_string)\n",
        "    #color: tuple (R,G,B) 0-255.  The mask is made by the closest key-color so you don't need to be exact\n",
        "    #blindfold: False or a float probability if how often to apply the blindfold (e.g. 0.9 means it will get blindfolded most of the time)\n",
        "    #   The blindfolding is to prevent that prompt from seeing other parts of the image which may influence. It isn't a hard blindfold, \n",
        "    #   rather the rest of the image is heavily blurred and noise is applied. It will still get some color information as a result\n",
        "\n",
        "\n",
        "    # spatial_prompts=[\n",
        "    #     ( (255,0,0), 0.8, '''the essence of spring'''),\n",
        "    #     ( (0,255,0), 0.8, '''the essence of summer'''),\n",
        "    #     ( (0,0,255), 0.8, '''the essence of autum'''),\n",
        "    #     ( (255,255,0), 0.8, '''the essence of winter'''),\n",
        "    #     ( (0,255,255), 0.8, '''magic energy ball'''),\n",
        "    # ],\n",
        "    spatial_prompts=[\n",
        "        ( (255,0,0), 0.2, '''a massive, dark, steampunk building filling the picture. a mass of steampunk. gray and black machine.'''),\n",
        "        ( (0,255,0), 0.5, '''a beautiful lush tree on a steampunk ledge'''),\n",
        "        ( (0,0,255), 0.7, '''a single small sliver of glowing moon in a blue sky'''),\n",
        "        ( (0,0,0), 0.9, '''clear skies above. nothing but blue.'''),\n",
        "    ],\n",
        "\n",
        "    #for consistent style cues, this gets appended to the end of each spatial prompt. Can be None\n",
        "    append_to_prompts = 'trending on artstation', \n",
        "\n",
        "    #optional start image (set to None if not using)\n",
        "    #local path or URL\n",
        "    init_image= 'https://i.ibb.co/syK6H66/tower-init.png',\n",
        "    init_weight= 0.5,\n",
        "\n",
        "    #This is how the prompt mask is defined. It is an RBG image\n",
        "    #local path or URL. defaults to init image if set to None\n",
        "    prompt_key_image = 'https://i.ibb.co/C0fyQFC/tower-mask.png',\n",
        "    #prompt_key_image = 'https://i.ibb.co/sFZHfMB/fourmask.png', #four quadrants. red,green,blue,yellow,  cyan center dot\n",
        "    #prompt_key_image = 'https://i.ibb.co/Xph568j/map.png',#two halves. left=red right=green\n",
        "\n",
        "\n",
        "    #Balance these for memory constraints\n",
        "\n",
        "    #output image size\n",
        "    size=[460,350],#[671,512],\n",
        "    # cutouts / crops (more cutn, higher quality)\n",
        "    cutn=12,#16\n",
        "    accum_grad_steps=5, #effectively make cutn bigger\n",
        "    cut_pow=1,\n",
        "\n",
        "    #how much to dilate the masks to cause overlap\n",
        "    dilate_masks = 9,\n",
        "\n",
        "    #set this to False to revert to normal VQGAN+CLIP (using the prompt below)\n",
        "    use_spatial_prompts=True,\n",
        "    prompts=[\"not used unless use_spatial_prompts is False\"],\n",
        "\n",
        "    cont=False, #Don't reset z. Allows beginning from previous spot/z with new prompts\n",
        "    \n",
        "    max_iter= 6000,\n",
        "\n",
        "    # clip model settings\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config='vqgan_imagenet_f16_16384.yaml',         \n",
        "    vqgan_checkpoint='vqgan_imagenet_f16_16384.ckpt',\n",
        "    step_size=0.1,\n",
        "    \n",
        "    \n",
        "    # display\n",
        "    display_freq=25,\n",
        "    seed=159,    #RANDOM SEED\n",
        "    use_augs = False,#these are not replicated with masks, don't use with spatial prompts\n",
        "    noise_fac= 0.1,\n",
        "    ema_val = 0.99,\n",
        "\n",
        "    record_generation=False, #set to True if you want video\n",
        "\n",
        "    # noise and other constraints\n",
        "    use_noise = None,\n",
        "    constraint_regions = False,#\n",
        "    \n",
        "    \n",
        "    # add noise to embedding\n",
        "    noise_prompt_weights = None,\n",
        "    noise_prompt_seeds = [149],#\n",
        "\n",
        "    # mse settings\n",
        "    mse_withzeros = True,\n",
        "    mse_decay_rate = 50,\n",
        "    mse_epoches = 5,\n",
        "    mse_quantize = True,\n",
        "\n",
        "    # end itteration\n",
        "    max_itter = -1,\n",
        ")\n",
        "\n",
        "mse_decay = 0\n",
        "if args.init_weight:\n",
        "  mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "# <AUGMENTATIONS>\n",
        "augs = nn.Sequential(\n",
        "    \n",
        "    K.RandomHorizontalFlip(p=0.5),\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "    )\n",
        "\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "image.save('init3.png')\n",
        "\n",
        "if args.use_spatial_prompts:\n",
        "    assert not args.use_augs\n",
        "    if not args.prompt_key_image:\n",
        "        args.prompt_key_image = args.init_image\n",
        "    \n",
        "    #append style prompt to all spatial prompts\n",
        "    if args.append_to_prompts:\n",
        "        new_prompts = []\n",
        "        for color,blind,prompt in args.spatial_prompts:\n",
        "            if prompt[-1]==' ':\n",
        "                prompt+=args.append_to_prompts\n",
        "            elif prompt[-1]=='.' or prompt[-1]=='|' or prompt[-1]==',':\n",
        "                prompt+=\" \"+args.append_to_prompts\n",
        "            else:\n",
        "                prompt+=\". \"+args.append_to_prompts\n",
        "            new_prompts.append( (color,blind,prompt) )\n",
        "        args.spatial_prompts = new_prompts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oBdzpIAAn8m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crRQdV3jPSvw"
      },
      "source": [
        "# Constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD9R97ygRN-0"
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "if args.constraint_regions and args.init_image:\n",
        "  \n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  toksX, toksY = args.size[0] // 16, args.size[1] // 16\n",
        "\n",
        "  pil_image = Image.open(fetch(args.init_image)).convert('RGB')\n",
        "  pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "\n",
        "  width, height = pil_image.size\n",
        "\n",
        "  d = ImageDraw.Draw(pil_image)\n",
        "  for i in range(0,width,16):\n",
        "      d.text((i+4,0), f\"{int(i/16)}\", fill=(50,200,100))\n",
        "  for i in range(0,height,16):\n",
        "      d.text((4,i), f\"{int(i/16)}\", fill=(50,200,100))\n",
        "\n",
        "  pil_image = TF.to_tensor(pil_image)\n",
        "\n",
        "  print(pil_image.shape)\n",
        "  for i in range(pil_image.shape[1]):\n",
        "    for j in range(pil_image.shape[2]):\n",
        "      if i%16 == 0 or j%16 ==0:\n",
        "        pil_image[:,i,j] = 0\n",
        "\n",
        "  # select region\n",
        "  c_h = [16,32]\n",
        "  c_w = [0,40]\n",
        "\n",
        "  c_hf = [i*16 for i in c_h]\n",
        "  c_wf = [i*16 for i in c_w]\n",
        "\n",
        "  pil_image[0,c_hf[0]:c_hf[1],c_wf[0]:c_wf[1]] = 0\n",
        "\n",
        "  TF.to_pil_image(pil_image.cpu()).save('progress.png')\n",
        "  display.display(display.Image('progress.png'))\n",
        "\n",
        "  z_mask = torch.zeros([1, 256, int(height/16), int(width/16)]).to(device)\n",
        "  z_mask[:,:,c_h[0]:c_h[1],c_w[0]:c_w[1]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7EDme5RYCrt"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Using device:', device)\n",
        "\n",
        "  \n",
        "\n",
        "if not args.cont:\n",
        "    ##########\n",
        "    #initialize the image\n",
        "\n",
        "    tv_loss = TVLoss() \n",
        "\n",
        "    model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "    perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "    mse_weight = args.init_weight\n",
        "\n",
        "    cut_size = perceptor.visual.input_resolution\n",
        "    print('cut_size = {}'.format(cut_size))\n",
        "    # e_dim = model.quantize.e_dim\n",
        "\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        e_dim = 256\n",
        "        n_toks = model.quantize.n_embed\n",
        "        z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "        z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "    else:\n",
        "        e_dim = model.quantize.e_dim\n",
        "        n_toks = model.quantize.n_e\n",
        "        z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "        z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "    make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "    f = 2**(model.decoder.num_resolutions - 1)\n",
        "    toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "    \n",
        "\n",
        "    if args.seed is not None:\n",
        "        torch.manual_seed(args.seed)\n",
        "\n",
        "    if args.init_image:\n",
        "        pil_image = Image.open(fetch(args.init_image)).convert('RGB')\n",
        "        pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "        pil_image = TF.to_tensor(pil_image)\n",
        "        if args.use_noise:\n",
        "            pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "        z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "    else:\n",
        "        \n",
        "        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "            z = one_hot @ model.quantize.embed.weight\n",
        "        else:\n",
        "            z = one_hot @ model.quantize.embedding.weight\n",
        "        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "    z = EMATensor(z, args.ema_val)\n",
        "\n",
        "    if args.mse_withzeros and not args.init_image:\n",
        "        z_orig = torch.zeros_like(z.tensor)\n",
        "    else:\n",
        "        z_orig = z.tensor.clone()\n",
        "\n",
        "\n",
        "    opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                    std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "########################\n",
        "#Get prompts set up\n",
        "\n",
        "if not args.use_spatial_prompts:\n",
        "  print('using prompts: ', args.prompts)\n",
        "  all_prompts = args.prompts\n",
        "else:\n",
        "\n",
        "  #Make prompt masks\n",
        "  prompt_key_image = Image.open(fetch(args.prompt_key_image)).convert('RGB')\n",
        "  prompt_key_image = np.asarray(prompt_key_image)\n",
        "\n",
        "  #Set up color->prompt map\n",
        "  color_to_prompt_idx={}\n",
        "  all_prompts=[]\n",
        "  blindfold=[]\n",
        "  for i,(color_key,blind,prompt) in enumerate(args.spatial_prompts):\n",
        "    all_prompts.append(prompt)\n",
        "    blindfold.append(blind)\n",
        "    color_to_prompt_idx[color_key] = i\n",
        "  \n",
        "  color_to_prompt_idx_orig = dict(color_to_prompt_idx)\n",
        "\n",
        "  #init the masks\n",
        "  prompt_masks = torch.FloatTensor(\n",
        "      len(args.spatial_prompts),\n",
        "      1, #color channel\n",
        "      prompt_key_image.shape[0],\n",
        "      prompt_key_image.shape[1]).fill_(0)\n",
        "\n",
        "  #go pixel by pixel and assign it to one mask, based on closest color\n",
        "  for y in range(prompt_key_image.shape[0]):\n",
        "      for x in range(prompt_key_image.shape[1]):\n",
        "          key_color = tuple(prompt_key_image[y,x])\n",
        "\n",
        "          if key_color not in color_to_prompt_idx:\n",
        "            min_dist=999999\n",
        "            best_idx=-1\n",
        "            for color,idx in color_to_prompt_idx_orig.items():\n",
        "              dist = abs(color[0]-key_color[0])+abs(color[1]-key_color[1])+abs(color[2]-key_color[2])\n",
        "              #print('{} - {} = {}'.format(color,key_color,dist))\n",
        "              if dist<min_dist:\n",
        "                min_dist = dist\n",
        "                best_idx=idx\n",
        "            color_to_prompt_idx[key_color]=best_idx #store so we don't need to compare again\n",
        "            idx = best_idx\n",
        "          else:\n",
        "            idx = color_to_prompt_idx[key_color]\n",
        "\n",
        "          prompt_masks[idx,0,y,x]=1\n",
        "\n",
        "  prompt_masks = prompt_masks.to(device)\n",
        "\n",
        "  #dilate masks to prevent possible disontinuity artifacts\n",
        "  if args.dilate_masks:\n",
        "    struct_ele = torch.FloatTensor(1,1,args.dilate_masks,args.dilate_masks).fill_(1).to(device)\n",
        "    prompt_masks = F.conv2d(prompt_masks,struct_ele,padding='same')\n",
        "\n",
        "  #resize masks to output size\n",
        "  prompt_masks = F.interpolate(prompt_masks,(toksY * 16, toksX * 16))\n",
        "\n",
        "  #make binary\n",
        "  prompt_masks[prompt_masks>0.1]=1\n",
        "\n",
        "  #rough display\n",
        "  if prompt_masks.size(0)>=3:\n",
        "    print('first 3 masks')\n",
        "    TF.to_pil_image(prompt_masks[0:3,0].cpu()).save('ex-masks.png')   \n",
        "    display.display(display.Image('ex-masks.png')) \n",
        "    if prompt_masks.size(0)>=6:\n",
        "      print('next 3 masks')\n",
        "      TF.to_pil_image(prompt_masks[3:6,0].cpu()).save('ex-masks.png')   \n",
        "      display.display(display.Image('ex-masks.png')) \n",
        "  \n",
        "  if any(blindfold):\n",
        "      #Set up blur used in blindfolding\n",
        "      k=13\n",
        "      blur_conv = torch.nn.Conv2d(3,3,k,1,'same',bias=False,padding_mode='reflect',groups=3)\n",
        "      for param in blur_conv.parameters():\n",
        "          param.requires_grad = False\n",
        "      blur_conv.weight[:] = 1/(k**2)\n",
        "\n",
        "      blur_conv = blur_conv.to(device)\n",
        "  else:\n",
        "      blur_conv = None\n",
        "\n",
        "  num_prompts = len(all_prompts)\n",
        "\n",
        "#Embed prompts\n",
        "pMs = []\n",
        "\n",
        "if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "for prompt in all_prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "    # pMs[0].embed = pMs[0].embed + Prompt(embed, weight, stop).embed.to(device)\n",
        "\n",
        "\n",
        "def synth(z, quantize=True):\n",
        "    if args.constraint_regions:\n",
        "      z = replace_grad(z, z * z_mask)\n",
        "\n",
        "    if quantize:\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "      else:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "\n",
        "    else:\n",
        "      z_q = z.model\n",
        "\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z.average, True)\n",
        "\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')   \n",
        "    display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "def ascend_txt():\n",
        "    global mse_weight\n",
        "\n",
        "    out = synth(z.tensor)\n",
        "\n",
        "    if args.record_generation:\n",
        "      with torch.no_grad():\n",
        "        global vid_index\n",
        "        out_a = synth(z.average, True)\n",
        "        TF.to_pil_image(out_a[0].cpu()).save(f'./vids/{vid_index}.png')#f'/content/vids/{vid_index}.png')\n",
        "    vid_index += 1\n",
        "\n",
        "\n",
        "    cutouts,cutout_coords = make_cutouts(out)\n",
        "\n",
        "    #TODO divide cutouts into seperate bins based on location to apply different prompts (pM) to\n",
        "\n",
        "    if args.use_augs:\n",
        "      cutouts = augs(cutouts)\n",
        "\n",
        "    if args.noise_fac:\n",
        "      facs = cutouts.new_empty([args.cutn, 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "      cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "\n",
        "    if args.use_spatial_prompts:\n",
        "        cutouts_detached = cutouts.detach() #used to prevent gradient for unmask parts\n",
        "        if blur_conv is not None:\n",
        "            #Get the \"blindfolded\" image by blurring then addimg more noise\n",
        "            facs = cutouts.new_empty([cutouts.size(0), 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "            cutouts_blurred = blur_conv(cutouts_detached)+ facs * torch.randn_like(cutouts_detached)\n",
        "\n",
        "        #get mask patches\n",
        "        cutout_prompt_masks = []\n",
        "        for (x1,x2,y1,y2) in cutout_coords:\n",
        "            cutout_mask = prompt_masks[:,:,y1:y2,x1:x2]\n",
        "            cutout_mask = resample(cutout_mask, (cut_size, cut_size))\n",
        "            cutout_prompt_masks.append(cutout_mask)\n",
        "        cutout_prompt_masks = torch.stack(cutout_prompt_masks,dim=1) #-> prompts X cutouts X color X H X W\n",
        "        \n",
        "        #apply each prompt, masking gradients\n",
        "        prompts_gradient_masked_cutouts = []\n",
        "        for idx,prompt in enumerate(pMs):\n",
        "            keep_mask = cutout_prompt_masks[idx] #-> cutouts X color X H X W\n",
        "            #only apply this prompt if one image has a (big enough) part of mask\n",
        "            if keep_mask.sum(dim=3).sum(dim=2).max()> cut_size*2:\n",
        "                \n",
        "                block_mask = 1-keep_mask\n",
        "\n",
        "                #compose cutout of gradient and non-gradient parts\n",
        "                if blindfold[idx] and ((not isinstance(blindfold[idx],float)) or blindfold[idx]>random.random()):\n",
        "                    gradient_masked_cutouts = keep_mask*cutouts + block_mask*cutouts_blurred\n",
        "                else:\n",
        "                    gradient_masked_cutouts = keep_mask*cutouts + block_mask*cutouts_detached\n",
        "                # if vid_index%100==0:\n",
        "                #     print('prompt {} cut and mask'.format(idx))\n",
        "                #     TF.to_pil_image(gradient_masked_cutouts[0].cpu()).save('ex-masks.png')   \n",
        "                #     display.display(display.Image('ex-masks.png')) \n",
        "                #     TF.to_pil_image(keep_mask[0].cpu()).save('ex-masks.png')   \n",
        "                #     display.display(display.Image('ex-masks.png')) \n",
        "                prompts_gradient_masked_cutouts.append(gradient_masked_cutouts)\n",
        "        cutouts = torch.cat(prompts_gradient_masked_cutouts,dim=0)\n",
        "    iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        \n",
        "        global z_orig\n",
        "        \n",
        "        result.append(F.mse_loss(z.tensor, z_orig) * mse_weight / 2)\n",
        "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "            if args.mse_quantize:\n",
        "              z_orig = vector_quantize(z.average.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n",
        "            else:\n",
        "              z_orig = z.average.clone()\n",
        "\n",
        "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "              mse_weight = mse_weight - mse_decay\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "            else:\n",
        "              mse_weight = 0\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "    \n",
        "    \n",
        "    if args.use_spatial_prompts:\n",
        "      for prompt_masked_iii,prompt in zip(torch.chunk(iii,num_prompts,dim=0),pMs):\n",
        "        result.append(prompt(prompt_masked_iii))\n",
        "    else:\n",
        "      for prompt in pMs:\n",
        "          result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "\n",
        "vid_index = 0\n",
        "def train(i):\n",
        "    if args.accum_grad_steps<2 or i%args.accum_grad_steps==0:\n",
        "        opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    \n",
        "    loss = sum(lossAll)/len(lossAll)\n",
        "    \n",
        "    if args.accum_grad_steps>1:\n",
        "        loss /= args.accum_grad_steps\n",
        "\n",
        "    loss.backward()\n",
        "    \n",
        "    if args.accum_grad_steps<2 or i%args.accum_grad_steps==args.accum_grad_steps-1:\n",
        "        opt.step()\n",
        "        z.update()\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while i <= args.max_iter:\n",
        "\n",
        "            train(i)\n",
        "\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "              z = EMATensor(z.average, args.ema_val)\n",
        "              opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT3hKb5gJUPq"
      },
      "source": [
        "##you must have record_generation set to True to make the video\n",
        "#%cd vids\n",
        "#\n",
        "#images = \"%d.png\"\n",
        "#video = \"./video.mp4\"\n",
        "#!ffmpeg -r 30 -i $images -crf 20 -s 640x512 -pix_fmt yuv420p $video\n",
        "#%cd ..\n",
        "\n",
        "#%cd vids\n",
        "#%rm *.png\n",
        "#%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiZMW3kAUD1f"
      },
      "source": [
        "delete all frames from folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsixT6gqJ8aY"
      },
      "source": [
        "#%cd vids\n",
        "#%rm *.png\n",
        "#%cd ..\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}